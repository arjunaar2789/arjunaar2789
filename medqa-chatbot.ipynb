{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9502507,"sourceType":"datasetVersion","datasetId":5783264},{"sourceId":9502520,"sourceType":"datasetVersion","datasetId":5783273},{"sourceId":9510182,"sourceType":"datasetVersion","datasetId":5788745}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T17:03:02.801659Z","iopub.execute_input":"2024-10-15T17:03:02.802377Z","iopub.status.idle":"2024-10-15T17:03:03.200330Z","shell.execute_reply.started":"2024-10-15T17:03:02.802337Z","shell.execute_reply":"2024-10-15T17:03:03.199378Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/trainjsonl/phrases_no_exclude_train.jsonl\n/kaggle/input/textbooks/First_Aid_Step1.txt\n/kaggle/input/textbooks/Neurology_Adams.txt\n/kaggle/input/textbooks/Gynecology_Novak.txt\n/kaggle/input/textbooks/InternalMed_Harrison.txt\n/kaggle/input/textbooks/Immunology_Janeway.txt\n/kaggle/input/textbooks/Biochemistry_Lippincott.txt\n/kaggle/input/textbooks/Histology_Ross.txt\n/kaggle/input/textbooks/Physiology_Levy.txt\n/kaggle/input/textbooks/Cell_Biology_Alberts.txt\n/kaggle/input/textbooks/First_Aid_Step2.txt\n/kaggle/input/textbooks/Pathology_Robbins.txt\n/kaggle/input/textbooks/Pediatrics_Nelson.txt\n/kaggle/input/textbooks/Surgery_Schwartz.txt\n/kaggle/input/textbooks/Pathoma_Husain.txt\n/kaggle/input/textbooks/Obstentrics_Williams.txt\n/kaggle/input/textbooks/Pharmacology_Katzung.txt\n/kaggle/input/textbooks/Psichiatry_DSM-5.txt\n/kaggle/input/textbooks/Anatomy_Gray.txt\n/kaggle/input/testjsonl/phrases_no_exclude_test.jsonl\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Directory where the textbooks are stored in .txt format\nbook_dir = '/kaggle/input/textbooks'\n\nbook_texts = []\n\n# Read each textbook from the directory\nfor book_file in os.listdir(book_dir):\n    if book_file.endswith('.txt'):\n        with open(os.path.join(book_dir, book_file), 'r', encoding='utf-8') as file:\n            book_texts.append(file.read())\n\n# Checking if books are loaded properly\nprint(f\"Loaded {len(book_texts)} books.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:03:04.118740Z","iopub.execute_input":"2024-10-15T17:03:04.119788Z","iopub.status.idle":"2024-10-15T17:03:05.321579Z","shell.execute_reply.started":"2024-10-15T17:03:04.119735Z","shell.execute_reply":"2024-10-15T17:03:05.320587Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loaded 18 books.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to split text into sections\ndef split_into_sections(text, section_length=1000):\n    return [text[i:i+section_length] for i in range(0, len(text), section_length)]\n\n# Applying the function to each textbook and flattening the list of sections\nbook_sections = [split_into_sections(book) for book in book_texts]\nbook_sections = [section for sublist in book_sections for section in sublist]  # Flatten the nested list\n\n# Check the number of sections\nprint(f\"Total sections created: {len(book_sections)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:03:05.323337Z","iopub.execute_input":"2024-10-15T17:03:05.323673Z","iopub.status.idle":"2024-10-15T17:03:05.471521Z","shell.execute_reply.started":"2024-10-15T17:03:05.323639Z","shell.execute_reply":"2024-10-15T17:03:05.470607Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Total sections created: 89142\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:03:05.472564Z","iopub.execute_input":"2024-10-15T17:03:05.472841Z","iopub.status.idle":"2024-10-15T17:03:18.283063Z","shell.execute_reply.started":"2024-10-15T17:03:05.472811Z","shell.execute_reply":"2024-10-15T17:03:18.281893Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.44.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sentence_transformers import SentenceTransformer, util\n\n#Creating a sparse retriever using tf idf weighting systemy\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(book_sections)\n\n#Creating a dense retriever using pre-trained model embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generating embeddings for each section\nbook_embeddings = model.encode(book_sections)\n\nprint(f\"Created embeddings of shape: {book_embeddings.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:03:18.285573Z","iopub.execute_input":"2024-10-15T17:03:18.285950Z","iopub.status.idle":"2024-10-15T17:06:40.828210Z","shell.execute_reply.started":"2024-10-15T17:03:18.285912Z","shell.execute_reply":"2024-10-15T17:06:40.827274Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc00b50588d8447595a744efa6a05493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f709dcc4b4bb48fabaeec667c076efaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a52bf5723d47d28d00f3490bdde2e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39499be823be4e63ab936bc06c126d11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e60ef5ddac74da3845b8ef242aa642b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92238fe4ed7744dfa6bea289434ab9c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd9fbba315946d1a87a9895369acab5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03e320ad251546f283871fe689905c07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4da8dadf00524ca694aa26aa3ffb371c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62a0b2ba5838421da2eb76bb3ee98aeb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d63d7b2800849c986baf941c18675fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2786 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9046e04dd5304243a76a1313453b51bf"}},"metadata":{}},{"name":"stdout","text":"Created embeddings of shape: (89142, 384)\n","output_type":"stream"}]},{"cell_type":"code","source":"def hybrid_retrieve(query, alpha=0.7, num_sections=5): \n    query_embedding = model.encode(query)\n    dense_scores = util.cos_sim(query_embedding, book_embeddings).flatten()\n    sparse_scores = vectorizer.transform([query]).dot(tfidf_matrix.T).toarray().flatten()\n    \n    # Combining scores\n    hybrid_scores = alpha * dense_scores + (1 - alpha) * sparse_scores\n    hybrid_scores = hybrid_scores.cpu().numpy()\n    \n    # Retrieving top N sections based on hybrid scores\n    top_indices = hybrid_scores.argsort()[-num_sections:][::-1]  # Get top sections\n    top_sections = [book_sections[i] for i in top_indices]\n    \n    return top_sections\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:06:40.830462Z","iopub.execute_input":"2024-10-15T17:06:40.831113Z","iopub.status.idle":"2024-10-15T17:06:40.837773Z","shell.execute_reply.started":"2024-10-15T17:06:40.831076Z","shell.execute_reply":"2024-10-15T17:06:40.836907Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\ndef answer_question_with_bert(question, alpha=0.5, num_sections=5, max_length=512, stride=128):\n    \n    # Retrieving relevant sections (from your hybrid retrieval system)\n    retrieved_sections = hybrid_retrieve(question, alpha, num_sections=num_sections)\n    \n    # Combining retrieved sections into a single context\n    context = \" \".join(retrieved_sections)\n    \n    answer = qa_pipeline(question=question, context=context, max_length=max_length, stride=stride)\n    \n    return answer['answer']\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:07:59.579098Z","iopub.execute_input":"2024-10-15T17:07:59.579520Z","iopub.status.idle":"2024-10-15T17:07:59.585434Z","shell.execute_reply.started":"2024-10-15T17:07:59.579455Z","shell.execute_reply":"2024-10-15T17:07:59.584510Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Example question from the dataset\nquestion = \"Long bones are tubular or cuboidal?\"\n\nanswer = answer_question_with_bert(question, alpha=0.7)\nprint(f\"Answer: {answer}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:14:15.820658Z","iopub.execute_input":"2024-10-15T17:14:15.821037Z","iopub.status.idle":"2024-10-15T17:14:21.899236Z","shell.execute_reply.started":"2024-10-15T17:14:15.821004Z","shell.execute_reply":"2024-10-15T17:14:21.898222Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"485c3c6cd68d4698a4f213c7ca41eebd"}},"metadata":{}},{"name":"stdout","text":"Answer: tubular\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\n# Load the test JSONL file\ntest_questions = []\nwith open('/kaggle/input/testjsonl/phrases_no_exclude_test.jsonl', 'r') as file:\n    for line in file:\n        test_questions.append(json.loads(line))\n\n# Check the first question\nprint(test_questions[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:08:26.053088Z","iopub.execute_input":"2024-10-15T17:08:26.053515Z","iopub.status.idle":"2024-10-15T17:08:26.114230Z","shell.execute_reply.started":"2024-10-15T17:08:26.053450Z","shell.execute_reply":"2024-10-15T17:08:26.113285Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"{'question': 'A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?', 'answer': 'Tell the attending that he cannot fail to disclose this mistake', 'options': {'A': 'Disclose the error to the patient and put it in the operative report', 'B': 'Tell the attending that he cannot fail to disclose this mistake', 'C': 'Report the physician to the ethics committee', 'D': 'Refuse to dictate the operative report'}, 'meta_info': 'step1', 'answer_idx': 'B', 'metamap_phrases': ['junior orthopaedic surgery resident', 'completing', 'carpal tunnel repair', 'department chairman', 'attending physician', 'case', 'resident', 'cuts', 'flexor tendon', 'tendon', 'repaired', 'complication', 'attending', 'resident', 'patient', 'fine', 'need to report', 'minor complication', 'not', 'patient', 'not', 'to make', 'patient worry', 'resident to leave', 'complication out', 'operative report', 'following', 'correct next action', 'resident to take']}\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate_model_on_test(test_questions):\n    correct = 0\n    total = len(test_questions)\n    \n    for entry in test_questions:\n        question = entry['question']\n        options = entry['options'] \n        correct_answer_idx = entry['answer_idx'] \n        \n        generated_answer = answer_question_with_bert(question)\n        \n        # Finding the closest match to the generated answer from the options\n        best_choice = find_closest_answer(generated_answer, options)\n        \n        # Comparing with the correct answer\n        if best_choice == correct_answer_idx:\n            correct += 1\n            \n    accuracy = correct / total\n    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:09:07.121455Z","iopub.execute_input":"2024-10-15T17:09:07.122374Z","iopub.status.idle":"2024-10-15T17:09:07.128702Z","shell.execute_reply.started":"2024-10-15T17:09:07.122329Z","shell.execute_reply":"2024-10-15T17:09:07.127519Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from difflib import SequenceMatcher\n\ndef find_closest_answer(generated_answer, options):\n    # Finding the best matching option based on similarity\n    best_choice = None\n    best_score = 0\n    \n    for choice, text in options.items():\n        score = SequenceMatcher(None, generated_answer, text).ratio()\n        if score > best_score:\n            best_choice = choice\n            best_score = score\n    \n    return best_choice\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:09:08.863984Z","iopub.execute_input":"2024-10-15T17:09:08.864382Z","iopub.status.idle":"2024-10-15T17:09:08.870047Z","shell.execute_reply.started":"2024-10-15T17:09:08.864342Z","shell.execute_reply":"2024-10-15T17:09:08.869061Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}