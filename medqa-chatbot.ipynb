{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9502507,"sourceType":"datasetVersion","datasetId":5783264},{"sourceId":9502520,"sourceType":"datasetVersion","datasetId":5783273},{"sourceId":9510182,"sourceType":"datasetVersion","datasetId":5788745}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T16:46:11.443193Z","iopub.execute_input":"2024-09-30T16:46:11.443684Z","iopub.status.idle":"2024-09-30T16:46:11.833948Z","shell.execute_reply.started":"2024-09-30T16:46:11.443643Z","shell.execute_reply":"2024-09-30T16:46:11.833002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Directory where the textbooks are stored in .txt format\nbook_dir = '/kaggle/input/textbooks'\n\nbook_texts = []\n\n# Read each textbook from the directory\nfor book_file in os.listdir(book_dir):\n    if book_file.endswith('.txt'):\n        with open(os.path.join(book_dir, book_file), 'r', encoding='utf-8') as file:\n            book_texts.append(file.read())\n\n# Checking if books are loaded properly\nprint(f\"Loaded {len(book_texts)} books.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:46:11.922914Z","iopub.execute_input":"2024-09-30T16:46:11.923216Z","iopub.status.idle":"2024-09-30T16:46:13.318749Z","shell.execute_reply.started":"2024-09-30T16:46:11.923184Z","shell.execute_reply":"2024-09-30T16:46:13.317428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to split text into sections\ndef split_into_sections(text, section_length=1000):\n    return [text[i:i+section_length] for i in range(0, len(text), section_length)]\n\n# Applying the function to each textbook and flattening the list of sections\nbook_sections = [split_into_sections(book) for book in book_texts]\nbook_sections = [section for sublist in book_sections for section in sublist]  # Flatten the nested list\n\n# Check the number of sections\nprint(f\"Total sections created: {len(book_sections)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:46:13.321791Z","iopub.execute_input":"2024-09-30T16:46:13.322531Z","iopub.status.idle":"2024-09-30T16:46:13.477602Z","shell.execute_reply.started":"2024-09-30T16:46:13.322473Z","shell.execute_reply":"2024-09-30T16:46:13.476361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:46:13.478885Z","iopub.execute_input":"2024-09-30T16:46:13.479202Z","iopub.status.idle":"2024-09-30T16:46:26.256146Z","shell.execute_reply.started":"2024-09-30T16:46:13.479169Z","shell.execute_reply":"2024-09-30T16:46:26.255016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sentence_transformers import SentenceTransformer, util\n\n#Creating a sparse retriever using tf idf weighting systemy\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(book_sections)\n\n#Creating a dense retriever using pre-trained model embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generating embeddings for each section\nbook_embeddings = model.encode(book_sections)\n\nprint(f\"Created embeddings of shape: {book_embeddings.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:46:26.257807Z","iopub.execute_input":"2024-09-30T16:46:26.258709Z","iopub.status.idle":"2024-09-30T16:49:45.998985Z","shell.execute_reply.started":"2024-09-30T16:46:26.258658Z","shell.execute_reply":"2024-09-30T16:49:45.997855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hybrid_retrieve(query, alpha=0.7, num_sections=5): \n    query_embedding = model.encode(query)\n    dense_scores = util.cos_sim(query_embedding, book_embeddings).flatten()\n    sparse_scores = vectorizer.transform([query]).dot(tfidf_matrix.T).toarray().flatten()\n    \n    # Combining scores\n    hybrid_scores = alpha * dense_scores + (1 - alpha) * sparse_scores\n    hybrid_scores = hybrid_scores.cpu().numpy()\n    \n    # Retrieving top N sections based on hybrid scores\n    top_indices = hybrid_scores.argsort()[-num_sections:][::-1]  # Get top sections\n    top_sections = [book_sections[i] for i in top_indices]\n    \n    return top_sections\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:55:07.497629Z","iopub.execute_input":"2024-09-30T16:55:07.498048Z","iopub.status.idle":"2024-09-30T16:55:07.505677Z","shell.execute_reply.started":"2024-09-30T16:55:07.498012Z","shell.execute_reply":"2024-09-30T16:55:07.504268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\n# Loading the pre-trained BERT model for Question Answering\nqa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\n# Example question\nquestion = \"What is the treatment for hypertension?\"\ncontext = \"Hypertension treatment includes lifestyle changes, such as diet modification, and medications to lower blood pressure.\"\n\n# Getting the answer using the BERT-based model\nresult = qa_pipeline(question=question, context=context)\nprint(f\"Answer: {result['answer']}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:57:09.580122Z","iopub.execute_input":"2024-09-30T16:57:09.580677Z","iopub.status.idle":"2024-09-30T16:57:16.021506Z","shell.execute_reply.started":"2024-09-30T16:57:09.580624Z","shell.execute_reply":"2024-09-30T16:57:16.020429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\ndef answer_question_with_bert(question, alpha=0.5, num_sections=5, max_length=512, stride=128):\n    \n    # Retrieving relevant sections (from your hybrid retrieval system)\n    retrieved_sections = hybrid_retrieve(question, alpha, num_sections=num_sections)\n    \n    # Combining retrieved sections into a single context\n    context = \" \".join(retrieved_sections)\n    \n    answer = qa_pipeline(question=question, context=context, max_length=max_length, stride=stride)\n    \n    return answer['answer']\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:57:49.291971Z","iopub.execute_input":"2024-09-30T16:57:49.292685Z","iopub.status.idle":"2024-09-30T16:57:49.299507Z","shell.execute_reply.started":"2024-09-30T16:57:49.292642Z","shell.execute_reply":"2024-09-30T16:57:49.298376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example question from the dataset\nquestion = \"What is the treatment for hypertension?\"\n\nanswer = answer_question_with_bert(question, alpha=0.7)\nprint(f\"Answer: {answer}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:58:04.111494Z","iopub.execute_input":"2024-09-30T16:58:04.111906Z","iopub.status.idle":"2024-09-30T16:58:08.707634Z","shell.execute_reply.started":"2024-09-30T16:58:04.111866Z","shell.execute_reply":"2024-09-30T16:58:08.706545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Load the test JSONL file\ntest_questions = []\nwith open('/kaggle/input/testjsonl/phrases_no_exclude_test.jsonl', 'r') as file:\n    for line in file:\n        test_questions.append(json.loads(line))\n\n# Check the first question\nprint(test_questions[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:58:11.359537Z","iopub.execute_input":"2024-09-30T16:58:11.359922Z","iopub.status.idle":"2024-09-30T16:58:11.396235Z","shell.execute_reply.started":"2024-09-30T16:58:11.359886Z","shell.execute_reply":"2024-09-30T16:58:11.395396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model_on_test(test_questions):\n    correct = 0\n    total = len(test_questions)\n    \n    for entry in test_questions:\n        question = entry['question']\n        options = entry['options'] \n        correct_answer_idx = entry['answer_idx'] \n        \n        generated_answer = answer_question(question)\n        \n        # Finding the closest match to the generated answer from the options\n        best_choice = find_closest_answer(generated_answer, options)\n        \n        # Comparing with the correct answer\n        if best_choice == correct_answer_idx:\n            correct += 1\n            \n    accuracy = correct / total\n    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:58:11.987768Z","iopub.execute_input":"2024-09-30T16:58:11.988207Z","iopub.status.idle":"2024-09-30T16:58:11.995453Z","shell.execute_reply.started":"2024-09-30T16:58:11.988164Z","shell.execute_reply":"2024-09-30T16:58:11.994248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from difflib import SequenceMatcher\n\ndef find_closest_answer(generated_answer, options):\n    # Finding the best matching option based on similarity\n    best_choice = None\n    best_score = 0\n    \n    for choice, text in options.items():\n        score = SequenceMatcher(None, generated_answer, text).ratio()\n        if score > best_score:\n            best_choice = choice\n            best_score = score\n    \n    return best_choice\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:58:12.653668Z","iopub.execute_input":"2024-09-30T16:58:12.654047Z","iopub.status.idle":"2024-09-30T16:58:12.660032Z","shell.execute_reply.started":"2024-09-30T16:58:12.654011Z","shell.execute_reply":"2024-09-30T16:58:12.659006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model on the test set\nevaluate_model_on_test(test_questions)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T16:58:13.307029Z","iopub.execute_input":"2024-09-30T16:58:13.307910Z","iopub.status.idle":"2024-09-30T16:58:30.135940Z","shell.execute_reply.started":"2024-09-30T16:58:13.307867Z","shell.execute_reply":"2024-09-30T16:58:30.134370Z"},"trusted":true},"execution_count":null,"outputs":[]}]}